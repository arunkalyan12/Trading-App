{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23029,"status":"ok","timestamp":1728929971287,"user":{"displayName":"Arunkalyan Muddamsetty","userId":"01024841283896307656"},"user_tz":-330},"id":"L4S0Oj54HjHm","outputId":"e2bd4041-d8c9-4bf0-a547-0a1026231dbf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6790659,"status":"ok","timestamp":1728936778635,"user":{"displayName":"Arunkalyan Muddamsetty","userId":"01024841283896307656"},"user_tz":-330},"id":"22u5PQIJHkoY","outputId":"779f6bc8-91d6-4491-fdbe-3b8024347ca6"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:947: UserWarning: Variables are collinear\n","  warnings.warn(\"Variables are collinear\")\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import yaml\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score\n","from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier, Perceptron, \\\n","    PassiveAggressiveClassifier\n","from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n","from sklearn.svm import SVC, LinearSVC\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, \\\n","    ExtraTreesClassifier, BaggingClassifier\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n","from xgboost import XGBClassifier\n","import os\n","\n","# Load configuration from YAML file\n","def load_config(config_path: str) -\u003e dict:\n","    with open(config_path, 'r') as file:\n","        config = yaml.safe_load(file)\n","    return config\n","\n","# Load and preprocess data\n","def main(config_path: str):\n","    config = load_config(config_path)\n","\n","    # Load the preprocessed data\n","    df = pd.read_csv('/content/drive/MyDrive/projects/Stock/Preprocessed.csv')\n","    df.dropna(inplace = True)\n","\n","    # Define features and labels based on the config\n","    X = df[config['data']['features']]  # Features based on the config\n","    y = df[config['data']['label']]  # Labels based on the config\n","    X = X.loc[:, ~X.columns.isin(['Open', 'Close Time'])]\n","\n","    # Train/Test split\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","    # Scaling features\n","    scaler = StandardScaler()\n","    X_train = scaler.fit_transform(X_train)\n","    X_test = scaler.transform(X_test)\n","\n","    # List of models to test\n","    models = {\n","        \"Logistic Regression\": LogisticRegression(),\n","        \"KNeighbors Classifier\": KNeighborsClassifier(),\n","        \"Decision Tree\": DecisionTreeClassifier(),\n","        \"Random Forest\": RandomForestClassifier(),\n","        \"Gradient Boosting\": GradientBoostingClassifier(),\n","        \"AdaBoost\": AdaBoostClassifier(),\n","        \"Extra Trees\": ExtraTreesClassifier(),\n","        \"Bagging\": BaggingClassifier(),\n","        \"MLP Classifier\": MLPClassifier(),\n","        \"SVC\": SVC(),\n","        \"Linear SVC\": LinearSVC(),\n","        \"Ridge Classifier\": RidgeClassifier(),\n","        \"SGD Classifier\": SGDClassifier(),\n","        \"GaussianNB\": GaussianNB(),\n","        \"Quadratic Discriminant Analysis\": QuadraticDiscriminantAnalysis(),\n","        \"Perceptron\": Perceptron(),\n","        \"Passive Aggressive\": PassiveAggressiveClassifier(),\n","        \"Nearest Centroid\": NearestCentroid(),\n","        \"XGBoost Classifier\": XGBClassifier()\n","    }\n","\n","    # Evaluate each model\n","    for name, model in models.items():\n","        if name == \"XGBoost Classifier\":\n","            # Remap the labels from [-1, 0, 1] to [2, 0, 1] for compatibility with XGBoost\n","            y_train_mapped = y_train.map({-1: 2, 0: 0, 1: 1})\n","            y_test_mapped = y_test.map({-1: 2, 0: 0, 1: 1})\n","            model.fit(X_train, y_train_mapped)\n","            y_pred = model.predict(X_test)\n","            accuracy = accuracy_score(y_test_mapped, y_pred)\n","        else:\n","            model.fit(X_train, y_train)\n","            y_pred = model.predict(X_test)\n","            accuracy = accuracy_score(y_test, y_pred)\n","\n","        with open('/content/drive/MyDrive/projects/Stock/model_performance_colab.txt', 'a') as txt:\n","            txt.write(f'{name} Accuracy: {accuracy:.4f}\\n')\n","\n","# Replace with your Google Drive path to the config file\n","if __name__ == \"__main__\":\n","    main('/content/drive/MyDrive/projects/Stock/config.yaml')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"BxjUWMCyJKwg"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-6-58031dcdeadd\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 84\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 85\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m\u003cipython-input-6-58031dcdeadd\u003e\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;31m# Fit and evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 64\u001b[0;31m             \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-\u003e 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1017\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1019\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1958\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1960\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1961\u001b[0m             ParameterSampler(\n\u001b[1;32m   1962\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    963\u001b[0m                     )\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 965\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    966\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    967\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         )\n\u001b[0;32m---\u003e 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 2007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1650\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1761\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-\u003e 1762\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import pandas as pd\n","import yaml\n","from xgboost import XGBClassifier\n","from sklearn.model_selection import RandomizedSearchCV, KFold\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, \\\n","    ExtraTreesClassifier, BaggingClassifier\n","\n","# Load configuration from YAML file\n","def load_config(config_path: str) -\u003e dict:\n","    with open(config_path, 'r') as file:\n","        config = yaml.safe_load(file)\n","    return config\n","\n","def main():\n","    try:\n","        # Load configuration\n","        config = load_config('/content/drive/MyDrive/projects/Stock/config.yaml')  # Specify the path to your config file\n","\n","        # Load preprocessed data directly from CSV file\n","        df = pd.read_csv('/content/drive/MyDrive/projects/Stock/Preprocessed.csv')\n","\n","        # Drop rows with missing values\n","        df.dropna(inplace=True)\n","\n","        # Define features and labels based on the config\n","        X = df[config['data']['features']]\n","        X = X.loc[:, ~X.columns.isin(['Open', 'Close Time'])]\n","        y = df[config['data']['label']]\n","\n","        # Hyperparameter space for Extra Trees\n","        param_space = {\n","            'n_estimators': [50, 100, 200, 500],\n","            'max_features': ['sqrt', 'log2', None],\n","            'max_depth': [None, 10, 20, 30, 40, 50],\n","            'min_samples_split': [2, 5, 10],\n","            'min_samples_leaf': [1, 2, 4],\n","            'bootstrap': [True, False]\n","        }\n","\n","        # Set up K-Fold cross-validation\n","        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","        best_accuracy = 0\n","        best_params = None\n","        results = []\n","\n","        # Randomized search with cross-validation\n","        for i in range(15):  # 15 iterations\n","            model = ExtraTreesClassifier()\n","\n","            # RandomizedSearchCV\n","            random_search = RandomizedSearchCV(\n","                estimator=model,\n","                param_distributions=param_space,\n","                n_iter=10,  # Number of parameter settings sampled\n","                cv=kf,\n","                scoring='accuracy',\n","                random_state=42,\n","                n_jobs=-1,\n","                verbose=1\n","            )\n","\n","            # Fit and evaluate\n","            random_search.fit(X, y)\n","            accuracy = random_search.best_score_\n","            params = random_search.best_params_\n","\n","            results.append((accuracy, params))\n","\n","            # Check if this is the best accuracy\n","            if accuracy \u003e best_accuracy:\n","                best_accuracy = accuracy\n","                best_params = params\n","\n","        # Log best parameters to a file\n","        with open('/content/drive/MyDrive/projects/Stock/best_extra_trees_params.txt', 'w') as f:\n","            f.write(f'Best Accuracy: {best_accuracy:.4f}\\n')\n","            f.write(f'Best Hyperparameters: {best_params}\\n')\n","\n","    except Exception as e:\n","        print(f'Error occurred: {e}')\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8aFu6oKvLxay"},"outputs":[],"source":["import pandas as pd\n","from xgboost import XGBClassifier\n","from sklearn.model_selection import RandomizedSearchCV, KFold\n","import yaml\n","\n","# Load configuration from YAML file\n","def load_config(config_path: str) -\u003e dict:\n","    with open(config_path, 'r') as file:\n","        config = yaml.safe_load(file)\n","    return config\n","\n","\n","def main():\n","    try:\n","        # Load configuration\n","        config = load_config('/content/drive/MyDrive/projects/Stock/config.yaml')\n","\n","        # Load preprocessed data directly from CSV file\n","        df = pd.read_csv('/content/drive/MyDrive/projects/Stock/Preprocessed.csv')\n","\n","        # Drop rows with missing values\n","        df.dropna(inplace=True)\n","\n","        # Define features and labels based on the config\n","        X = df[config['data']['features']]\n","        X = X.loc[:, ~X.columns.isin(['Open', 'Close Time'])]\n","        y = df[config['data']['label']]\n","\n","        # Map the target labels: -1 to 2, 0 to 0, and 1 to 1\n","        y_mapped = y.map({-1: 2, 0: 0, 1: 1})\n","\n","        # Convert categorical columns to numerical (if applicable)\n","        # For example, using one-hot encoding\n","        X = pd.get_dummies(X, drop_first=True)\n","\n","        # Hyperparameter space for XGBoost\n","        param_space = {\n","            'n_estimators': [50, 100, 200, 500],\n","            'max_depth': [3, 5, 7, 10],\n","            'learning_rate': [0.01, 0.1, 0.2],\n","            'subsample': [0.5, 0.7, 1],\n","            'colsample_bytree': [0.5, 0.7, 1],\n","            'gamma': [0, 0.1, 0.2]\n","        }\n","\n","        # Set up K-Fold cross-validation\n","        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","        best_accuracy = 0\n","        best_params = None\n","        results = []\n","\n","        # Randomized search with cross-validation\n","        for i in range(15):  # 15 iterations\n","            model = XGBClassifier(\n","                n_estimators=100,          # or your desired value\n","                max_depth=5,               # or your desired value\n","                learning_rate=0.1,         # or your desired value\n","                subsample=0.8,             # or your desired value\n","                colsample_bytree=0.8,      # or your desired value\n","                gamma=0,                   # or your desired value\n","                tree_method=\"hist\",        # or \"approx\" for large datasets\n","                )  # Specify GPU usage\n","\n","            # RandomizedSearchCV\n","            random_search = RandomizedSearchCV(\n","                estimator=model,\n","                param_distributions=param_space,\n","                n_iter=10,\n","                cv=kf,\n","                scoring='accuracy',\n","                random_state=42,\n","                n_jobs=-1,\n","                verbose=1\n","            )\n","\n","            # Fit and evaluate\n","            random_search.fit(X, y_mapped)  # Use the mapped labels\n","            accuracy = random_search.best_score_\n","            params = random_search.best_params_\n","\n","            results.append((accuracy, params))\n","\n","            # Check if this is the best accuracy\n","            if accuracy \u003e best_accuracy:\n","                best_accuracy = accuracy\n","                best_params = params\n","\n","        # Log best parameters to a file\n","        with open('/content/drive/MyDrive/projects/Stock/best_xgboost_params.txt', 'w') as f:\n","            f.write(f'Best Accuracy: {best_accuracy:.4f}\\n')\n","            f.write(f'Best Hyperparameters: {best_params}\\n')\n","\n","    except Exception as e:\n","        print(f'Error occurred: {e}')\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_RsnYRSu_zpn"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyP0ni4dB9wtWn28vpKDfGzt","mount_file_id":"1JnYqfbeeyEOuGnLVJdUztvy7c8_I8l5k","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}